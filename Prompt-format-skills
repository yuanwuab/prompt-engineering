# Prompt Framework and Skills

## Co-STAR Framework

**(C) Context**
*Provide background information on the task*
This helps the LLM understand the specific scenario being discussed, ensuring its response is relevant.

**(O) Objective**
*Define what the task is that you want the LLM to perform*
Being clear about your objective helps the LLM to focus its response on meeting that specific goal.

**(S) Style**
*Specify the writing style you want the LLM to use*
This could be a particular famous person’s style of writing, or a particular expert in a profession, like a business analyst or CEO. This guides the LLM to respond in the manner and tone aligned with your needs.

**(T) Tone**
*Set the attitude of the response*
This ensures the LLM’s response resonates with the intended sentiment or emotional context required. Examples include formal, humorous, empathetic, among others.

**(A) Audience**
*Identify who the response is intended for*
Tailoring the LLM’s response to an audience—such as experts, beginners, or children—ensures that it is appropriate and understandable.

**(R) Response**
*Provide the response format*
This ensures that the LLM outputs in the exact format required for downstream tasks. Examples include a list, JSON, or professional report. For most LLM applications that process LLM responses programmatically, a JSON output format is ideal.

---

## Sectioning Prompts Using Delimiters

### Delimiters as Special Characters

A delimiter could be any sequence of special characters that usually wouldn’t appear together, for example:

```
###
===
>>>
```

The number and type of special characters chosen is inconsequential, as long as they are unique enough for the LLM to recognize them as content separators.

### Delimiters as XML Tags

Another approach to using delimiters is using **XML-style tags** such as `<tag>` and `</tag>`.
LLMs are well-trained on XML and can easily interpret these formats as structural cues.

---

## Creating System Prompts With LLM Guardrails

### What are System Prompts?

System Prompts are special instructions that guide how the LLM should behave.
They differ from user prompts in that they provide **persistent behavioral instructions** across the session.

### When Should System Prompts Be Used?

Use System Prompts when you want the LLM to remember consistent instructions throughout the chat.

### What Should System Prompts Include?

System Prompts typically include:

* **Task Definition**: So the LLM knows its ongoing objective.
* **Output Format**: So the LLM knows how to respond.
* **Guardrails**: Boundaries defining how the LLM should *not* respond. Guardrails help enforce safe, consistent usage.

---

## Analyzing Datasets Using Only LLMs (Without Plugins or Code)

### Tasks That LLMs Are *Not* Great At:

* **Descriptive Statistics**
  Calculating mean, variance, and other summary stats precisely.

* **Correlation Analysis**
  Computing correlation coefficients between variables.

* **Statistical Analysis**
  Performing hypothesis tests or p-value evaluations.

* **Machine Learning**
  Running predictive models like regressions or neural networks.
